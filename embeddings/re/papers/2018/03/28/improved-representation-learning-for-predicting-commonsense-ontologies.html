<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>[Paper] Improved Representation Learning for Predicting Commonsense Ontologies</title>
  <meta name="description" content="Improved Representation Learning for Predicting Commonsense Ontologies">

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://localhost:4000/embeddings/re/papers/2018/03/28/improved-representation-learning-for-predicting-commonsense-ontologies.html">
  <link rel="alternate" type="application/rss+xml" title="Tim Niven (寒山)" href="/feed.xml">
  
  
</head>


  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">

    <a class="site-title" href="/">Tim Niven (寒山)</a>

    <nav class="site-nav">
      <span class="menu-icon">
        <svg viewBox="0 0 18 15" width="18px" height="15px">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </span>

      <div class="trigger">
        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
          <a class="page-link" href="/publications/">Publications</a>
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">[Paper] Improved Representation Learning for Predicting Commonsense Ontologies</h1>
    <p class="post-meta"><time datetime="2018-03-28T00:00:00+08:00" itemprop="datePublished">Mar 28, 2018</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <h1 id="improved-representation-learning-for-predicting-commonsense-ontologies">Improved Representation Learning for Predicting Commonsense Ontologies</h1>

<p>Li et al. (2017)</p>

<p>https://arxiv.org/abs/1708.00549</p>

<h2 id="problem-and-solution">Problem and Solution</h2>

<ul>
  <li>“Prediction” in the sense of the article is predicting the layout of embeddings in a space</li>
  <li>We are concerned principally here with hierarchical data</li>
  <li>We want to learn embeddings that facilitate <strong>reasoning</strong> so an we can infer missing knowledge from what we have</li>
  <li>Therefore the embeddings must lend themselves to things such as transitivity and negation</li>
  <li>So if a dog is a mammal and a pit bull is a dog, we want to infer that a pit bull is a mammal</li>
  <li>Most previous models only focus on “local prediction”: enforcing consistency between localized entities in the space - e.g. semantically similar words are close together</li>
  <li>Some recent research has started to work on “global prediction”, where the geometry of the space is used to enforce consistency - e.g. Poincare embeddings in hyperbolic space, a continuous version of trees, for enforcing hierarchical consistency</li>
  <li>This work focuses on the order-embedding model Vendrov et al. (2016), which it is claimed can:
    <ul>
      <li>complex ordering structure</li>
      <li>compositional, multi-word entities</li>
    </ul>
  </li>
  <li>The order embedding model is trained, like many others, from pairs of training examples, drawn from a known hierarchy</li>
  <li>This work extends this in two ways:
    <ul>
      <li>adding non-hierarchical information in the form of raw text</li>
      <li>adding longer distance triplet constraints not present in the training pairs</li>
    </ul>
  </li>
</ul>

<h2 id="background">Background</h2>

<ul>
  <li>Knowledge graph completion (Bordes et al. 2013, Wang et al. 2014, Nickel et al. 2011, Li et al. 2016, Socher et al. 2013)</li>
  <li>Global prediction (Vilnis &amp; McCallum 2015, Vendrov et al. 2016, Nickel &amp; Kiela 2017)</li>
</ul>

<h2 id="datasets">Datasets</h2>

<ul>
  <li>ConceptNet (Speer et al. 2016)</li>
  <li>WordNet (Miller 1995)</li>
  <li>Microsoft Concept Graph (Wu et al 2012, Want et al. 2015)</li>
</ul>

<h2 id="order-embeddings">Order Embeddings</h2>

<p>The space is constrained such that higher-level concepts $y$ are smaller in every coordinate than lower level concepts $x$</p>

<script type="math/tex; mode=display">x \preceq y \ \text{iff} \ \bigwedge_{i=1}^N x_i \ge y_i</script>

<p>This can be visualized thus</p>

<p><img src="attachment:image.png" alt="image.png" /></p>

<p>An energy for this ordering function is</p>

<script type="math/tex; mode=display">d(x,y) = ||\max(0, y - x)||</script>

<p>leading to a ranking loss objective</p>

<script type="math/tex; mode=display">L_{\text{Order}} = \sum_{x,y} \max(0, m + d(x, y) - d(x', y'))</script>

<p>where $’$ indicates negative samples.</p>

<p>Notes:</p>
<ul>
  <li>Looks like vectors are constrained to the positive domain</li>
  <li>This is strikingly similar to “Lifted Rule Injection” where they impose the same structural constraint for more and less general entailment rules.</li>
</ul>

<h2 id="joint-text-and-order-embedding">Joint Text and Order Embedding</h2>

<p>A modified CBOW is used to augment the order embedding with signal from raw text. An average embedding $v_2$ from words in the context window $w$ is used to predict the center word $v_1$</p>

<script type="math/tex; mode=display">v_2 = \frac{1}{w} \sum_{k \in \{ -w,\dots,w \}\backslash\{t\}} v_{t + k}</script>

<p>Since order embeddings are all positive and compared componentwise $L_1$ is used to measure distance from the context vector, not dot product, yielding the loss</p>

<script type="math/tex; mode=display">L_{\text{CBOW}} = \sum_{w_c, w_t} \max(0, m + ||v_1 - v_2|| - ||v_1' - v_2'||)</script>

<p>The final loss is then a weighted sum</p>

<script type="math/tex; mode=display">L_{\text{joint}} = \alpha_1 L_{\text{Order}} + \alpha_2 L_{\text{CBOW}}</script>

<p>This yields an accuracy increase from 92.0 to 93.0 on ConceptNet and MCG IS-A relations.</p>

<h2 id="long-range-join-and-meet-constraints">Long Range Join and Meet Constraints</h2>

<p>Using the information in the hierarchy we can augment the training set by including constraints from transitive cases. If we have a pair of words $w_1$ and $w_2$ as in the following figure, we find the nearest common parent $w_p$ and child $w_c$ and enforce the desired ordering</p>

<p><img src="attachment:image.png" alt="image.png" /></p>

<p>To enforce the constraint, we consider the “join” $\lor$ and “meet” $\land$ of the word pairs. The join is the pointwise max, and the meet is the pointwise min of the two vectors (clear in the figure). Then we have</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
d_c(w_1, w_2, w_c) &= ||\max(0), w_1 \lor w_2 - w_c||^2 \\
d_p(w_1, w_2, w_p) &= ||\max(0), w_p - w_1 \land w_2||^2 \\
L_{\text{join}} &= \sum{w_1, w_2, w_c} \max(0, m + d_c(w_1, w_2, w_c)) \\
L_{\text{meet}} &= \sum{w_1, w_2, w_p} \max(0, m + d_p(w_1, w_2, w_p)) \\
L &= L_{\text{join}} + L_{\text{meet}} \\
\end{align*} %]]></script>

<p>Adding the join and meet constraints leads to a performance increase on the WordNet ontology from 90.6 to 91.3 accuracy.</p>

  </div>

  
</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <!--<h2 class="footer-heading">Tim Niven (寒山)</h2>-->

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>
            
              Tim Niven (寒山)
            
            </li>
            
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/timniven"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">timniven</span></a>

          </li>
          

          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>I maintain this blog to share information pertaining to my research that may be of interest to others, as well as generally useful information I wish to make a note of.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
