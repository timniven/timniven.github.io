---
layout: post
title: "[Paper] Tracking the World State with Recurrent Entity Networks"
date: 2018-03-08
categories: papers memory-networks
---


# Tracking the World State with Recurrent Entity Networks

Henaff et al. (2017)

https://arxiv.org/pdf/1612.03969.pdf

## Problem and Solution

- Intelligence requires storing the state of the world, and having a model for how it will evolve in the future, in order to make predictions
- The nature of the tasks (babi QA and Children's Story) provide the setting: sequences of statements about the state of the world that evolve through time, punctuated with questions that provide supervision
- From this supervision signal we aim to learn:
  1. basic dynamical constraints, such as that somethings cannot be in two places at once
  2. simple update rules, such as decrementing the number of people in a room after someone leaves
  3. basic approximate inference, such as inferring that objects belonging to the same category have similar properties (e.g. light objects can be carried from room to room)
- Entities are modeled as dynamic memory cells that can be updated after reading new information
- These cells do not communicate, reflecting the invariance of the rules that should apply to their updating
- The memory module can be thought of as a bank of RNN cells whose hidden states are latent concepts and attributes, and whose parameters describe the rules of their updating

## Related Work

Compared models for evaluation (babi QA):
- Sukhbaatar et al. (2015) MemN2N
- Xiong et al. (2016) Dynamic Memory Network
- Gulcehre et al. (2016) Dynamic Neural Turing Machine
- Graves et al. (2014) Neural Turing Machine
- Graves et al. (2016) Differentiable Neural Computer

Memory Networks (and variants):
- Weston et al. (2014)
- Sukhbaatar et al. (2015)
- Chandar et al. (2016)
- Miller et al. (2016)
- Xiong et al. (2016)

Graph Network:
- Li et al. (2015)

Writeable memory as stacks, linked lists, or queues:
- Joulin & Mikolov (2015)
- Grefenstette et al. (2015)

## Datasets

- babi QA (Weston et al. 2015)
- babi Children's Book Test (Hill et al. 2016)
- synthetic task to test reasoning with a large number of supporting facts

## Model

- Differences with LSTM/GRU:
  - also uses gating to update the hidden state, however it uses a common weight for an entire group (cell)
  - "content-based matching tern between input and hidden state" (LOOK AT FURTHER)
- Differences with NTM:
  - NTM uses a "controller" to read and write to an external memory matrix, whereas here the memory cells are more like RNN hidden states
  - No need to compute a softmax to update memories in this model
- Differences with Memory Network (and variants):
  - There have sequential updating via softmax over memories, whereas here a fixed number of hidden blocks are updated with indepdendent RNNs
  - Xiong et al. (2016) link tokens to memories and update them sequentially (recurrently), but here memory cells are entities/concepts and are updated in parallel

### 1. Input Encoder

Inputs from babi QA come as sentences at each timestep, for which we obtain a vector representation

$$ s_t = \sum_i f_i \odot e_i  \quad f_i, e_i \in \Bbb{R}^{d_e} $$

where $e_i$ are word vectors and $f_i$ are multiplicative mask vectors (looks like one for each word), and the dimension of the word embeddings is $d_e$.

### 2. Dynamic Memory

We have $m$ memory cells $h_j$, each updated in parallel when a new sentence $s_t$ arrives by

$$
\begin{align*}
g_j &\leftarrow \sigma (s_t^Th_j + s_t^T w_j) \\
\tilde{h}_j &\leftarrow \phi (Uh_j + Vw_j + Ws_t) \\
h_j &\leftarrow h_j + g_j \odot \tilde{h}_j \\
h_j &\leftarrow \frac{h_j}{||h_j||}
\end{align*}
$$

Here
- $g_j$ is a gating function that determines how much $h_j$ should be updated
- $\tilde{h}_j$ is the candidate update
- $\phi$ is PReLU or identity in their work
- $w_j$ is a key vector associated with each memory state.

The gating function has two terms
- "content" term, $s_t^Th_j$, causes the memory gate to open for memory slotes whose content matches the input
- "location" term, $s_t^Tw_j$, causes the gate to open for memory slots whose key matches the input

The final model step allows the model to forget previous information:
- all memories lie on the unit sphere
- information is thereby given exclusively by angle
- vector addition closes the angular distance between the original and subsequent memories

### 3. Output Module

Given a query vector $q$ the output is generated by

$$
\begin{align*}
p_j &= \text{softmax}(q^Th_j) \\
u &= \sum_j p_jh_j \\
y &= R\phi(q + Hu)
\end{align*}
$$

Think of $R$ as a decoder matrix - we want to translate whatever we have stored in memory into the output space.

If the memory slots contain all possible answers (e.g. words) then the last two steps are unnecessary and a prediction can be delivered directly from $p$ which would then be a distribution over potential answers.

## Discussion

What is this model learning, and how does it learn it?

- The model is free to learn the keys $w_j$
- It may choose to encode each entity in the story in a cell
- Each cell then holds mutable information about that entity's state - e.g. holding a ball, in the kitchen, what have you
- This structure could be forced on the model - e.g. associating word vectors with a memory slots, or named entities (perhaps identified by a standard tagger)
- The latter option looks like an advantage for generalization to unseen entities

## Issues

- As best I can see just now, the QA sets in a batch of 32 can't be processed in parallel, since if question 1 has "Mary goes to the kitchen" and question 25 says "Mary goes to the bathroom" at the same timestep, this will create an update conflict. So it appears they need to be processed linearly, which reduces the efficiency of this model. I guess the other option would be to create 32 copies of the parameters and then aggregate the gradient. I won't go that path.
- I have interpreted the $W$ in the dynamic memory update equations as a <strong>separate</strong> parameter from the matrix holding the keys $w_j$.
- Can't see the detail, going for $\phi = \text{PReLU}$ in the output module.
- With that final equation in the output module, do we need to normalize with softmax? They don't give details of loss functions of the like...
- Didn't notice any biases in the equations - didn't note any mention of them, assume they are not there

## Thoughts

- I like the discussion of linking memory keys to word vectors - the next step would be real world entities (named entity recognition) that would make it applicable to real world text (going beyond toy tasks)
- Can this entity recognition and memory addressing be learned jointly?
- Memories should be affected by broader world knowledge - another component to integrate