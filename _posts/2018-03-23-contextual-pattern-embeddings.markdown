---
layout: post
title: "[Paper] Contextual Pattern Embeddings"
date: 2018-02-09
categories: maxent
---

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


# Contextual Pattern Embeddings

Obamuyide and Vlachos (2017), NIPS

http://www.akbc.ws/2017/papers/17_paper.pdf

## Problem

- To learn extractors for new relations for which there is limited training data (one-shot learning)
- With limited training data there is limited supervision saying what relations match what surface linguistic patterns
- A complication is "semantic drift": since model predictions are used for subsequent predictions, noise in these predictions can put the whole process off course

## Solution

- Factorization Machine model
- Uses embeddings of the surface linguistic patterns as well as relations and entities (see Figure 1 on p.3). Actually I think that figure is very clear in showing how correlations between surface patterns and relations can be easily captured by the model
- Universal Schema, which learns a joint embedding space for surface linguistic patterns and relations, sidesteps the problem of aligning relations to surface patterns in the corpus by performing joint inference across surface patterns, relations, and entities
- The learning algorithm needs to see negative facts. However, assuming facts not in the seed KB are false runs an obvious risk. Following Riedel et al. (2013) they treat unobserved facts as unknowns that need to be inferred, and use a ranking objective that ranks observed facts higher than unobserved ones - Bayesian Personalized Ranking

## Details

### Factorization Machines

$$
s(\mathbf{f}) = \sum_{m=1}^d b_mf_m + \sum_{m=1}^d \sum_{n = m+1}^d \langle \phi_m, \phi_n \rangle f_m f_n
$$

It appears $d$ is the dimension of the sparse embedding of Figure 1. So each feature gets its own weight in the first term, which is a linear function. For the second term, $\phi_m \in \Bbb{R}^k$ with $k << d$ are learned vector representations for each feature.

### Proposed Approach

Represent facts as a triple $(r, t, c^t)$, being a relation, an entity pair, and a vector of counts of contextual patterns (surface patterns) observed together with the entity pairs in a text corpus, normalized to sum to one. THen $\mathbf{f}$ is represented by the concatenation of these vectors.

Relation and tuples are encoded as one-hot vectors. (Note: since tuples are pairs of entities, shouldn't this be a two-hot vector?) These vectors are therefore very sparse. However, they only sum over the active features $\mathcal{A}$

$$
s(\mathbf{f}) = \sum_{a \in A} b_af_a + \sum_{a \in A, a' \in A \backslash a} \langle \phi_a, \phi_{a'} \rangle f_a f_{a'}
$$

Bayesian personalized ranking of observed facts $F^+$ and unobserved $F^-$ is then

$$
\arg \min_\Theta - \sum_{f^+ \in F^+, f^- \in F^-} \log \left( 1 + \exp(s(f^+) - s(f^-)) \right) + \lambda \lVert \Theta \rVert^2
$$

The $F^-$ are generated by random sampling: for each positive fact, fix the relation and randomly select an entity pair such that the triple has not been observed.

## Training Details

- $d = 100$
- L2 of 0.01
- Trained for 1,000 epochs
- ADam with learning rate of $1 \times 10^{-4}$
- Batch size 1,024
- Random sample one negative fact per positive fact
- Shortest dependency path between each pair of entities in a sentence is extracted as the surface pattern
- In one-shot experiments, evaluations are performed with a fraction of the training labels for each relation $\tau \in [0, 0.5]$
- NER, linking, parsing are kept the same as Riedel et al. (2013) for a fair comparison

## Evaluation

- For each relation the top 1,000 entity pairs are retrieved, the top 100 of which are pooled and manually annotated, which are used to compute MAP and wMAP for each run.
- wMAP takes into account the number of true facts for each relation

## Background Knowledge and Related Work

- Matrix Factorization model
  - Rendle 2010
  - Riedel et al. 2013
  - Petroni et al. 2015
  - Weibl et al. 2016
- Universal Schema
  - Riedel et al. 2013
- IE related
  - Zelenko et al. 2003
  - Culotta and Sorenson 2004
  - Bunescu and Mooney 2006
  - Mintz et al. 2009
  - Surdeanu et al. 2012
  - Riedel et al. 2013
- One-shot learning (these appear to be about vision)
  - Miller et al. 2000
  - Fei-Fei et al. 2006
- Zero-shot learning
  - Larochelle et al. 2008
- Bootstrapped KB population
  - Carlson et al. 2010
- Semantic drift problem
  - Curran et al. 2007
- Bayesian Personalized Ranking (BPR)
  - Rendle et al. 2009
- Injecting logical rules into embeddings
  - Rocktashcel et al. 2015
  - Demeester et al. 2016
- Other compared models
  - Mintz et al. 2009
  - Yao et al. 2011
  - Surdeanu et al. 2012
- Compositional representations of surface patterns
  - Toutanova et al. 2015
  - Verga et al. 2016

## Discussion

- Surface patterns are decided by dependency paths, maybe we could do this as a part of a jointly trained system - use RL parsing a la SPINN (I think that's the name - Bowman's crowd), compose those vectors as opposed to the tabular format reminiscent of LSA used in this work

## Next

- That Riedel et al. 2013 paper shows up everywhere, must be read
- It also introduces the evaluation dataset - obtain that dataset