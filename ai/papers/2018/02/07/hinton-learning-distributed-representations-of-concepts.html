<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>[Paper] Learning Distributed Representations of Concepts - Hinton (1986)</title>
  <meta name="description" content="">

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://localhost:4000/ai/papers/2018/02/07/hinton-learning-distributed-representations-of-concepts.html">
  <link rel="alternate" type="application/rss+xml" title="Tim Niven (寒山)" href="/feed.xml">
  
  
</head>


  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">

    <a class="site-title" href="/">Tim Niven (寒山)</a>

    <nav class="site-nav">
      <span class="menu-icon">
        <svg viewBox="0 0 18 15" width="18px" height="15px">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </span>

      <div class="trigger">
        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
          <a class="page-link" href="/publications/">Publications</a>
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">[Paper] Learning Distributed Representations of Concepts - Hinton (1986)</h1>
    <p class="post-meta"><time datetime="2018-02-07T00:00:00+08:00" itemprop="datePublished">Feb 7, 2018</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<h2 id="summary">Summary</h2>

<p>If we take a conceptual relation such as “born-in”, we find that the two
entities participating in the relation should have common features: we
have a person and a place.</p>

<p>The idea in this paper is to represent the concepts that participate in
this kind of relation with a set of features. We let neural nets figure
out what those features should be by partitioning the space of entities
in such a way as to express the regularities of the domain. We use an
extra set of neurons to perform these “micro-inferences”. The example
given is: suppose we see a person with activated feature “old”, and the
relation “husband-of”, and suppose the regularities of our domain are
such that people are married to those of roughly the same age. From this
we should be able to infer that the other person in the relation should
also have “old” activated. The partition found is then that of “old” people.
Also, note that we can in this way handle lossy input and recall relevant
information.</p>

<p>An interesting theoretical insight into meaning and truth conditions follows
from this partitioning view. Philosophically, we don’t know what a meaning
is. However, truth values get us at least some of the way. It is possible
to ask: yes, the neuron fires for old, but does the network really know what old
means? In response, if we can say that it partitions the set of all
entities along lines we recognize, then it has captured this important
aspect of the meaning of old.</p>

<h2 id="two-theories-of-representation">Two Theories of Representation</h2>

<p>How do we represent concepts?</p>

<ul>
  <li>extreme localist: concept = neuron</li>
  <li>extreme distributed: concept = pattern of activity over large part of cortex</li>
</ul>

<p>Accord with main approaches to semantics:</p>

<ul>
  <li>structuralist (concepts are defined by their relations): encode this
in the connections between atomized neurons with no internal structure</li>
  <li>componential (concepts are sets of features): each feature is a neuron,
set connection weights such that a concept is then a stable pattern of
activity over the whole network
    <ul>
      <li>robust to noise</li>
      <li>how to use this for structured reasoning?</li>
      <li>good for concept similarity and pairwise associations</li>
      <li>“no obvious way of representing articulated structures composed of a
number of concepts playing different roles within that structure”</li>
    </ul>
  </li>
</ul>

<h2 id="role-specific-units">Role-Specific Units</h2>

<p>We are considering a structure such as:</p>

<p>John (agent) hit (relation) Mary (patient).</p>

<p>Suggestion:</p>

<ul>
  <li>assign a group of units to each role (agent, relation, patient)</li>
  <li>patterns of activity in that group represent the concepts filling the role</li>
  <li>each unit then represents the conjunction of a role with a feature of
the concept playing the role (e.g. one unit may be active if the filler
is male)</li>
  <li>the same concept then has different representations when playing
different roles</li>
</ul>

<p>The neurons for the role and the neurons for the concept are different.
The “conjunction” occurs via some kind of composition function.</p>

<p>Potential drawbacks and replies:</p>

<ul>
  <li>uneconomical to have multiple representations of a single entity
    <ul>
      <li>yes but having a single representation is less efficient for retrieval
of, say, all instances where John is the agent. A similar argument applies
to word searches. If we have “c-ip” and want all options for filling
in the blank it is inefficient to search all words that have c, i, and p.
Rather, accounting for their position (like role in our other examples)
will make it more efficient.</li>
    </ul>
  </li>
  <li>unclear that the different representations refer to the same entity -
e.g. “John hit Mary”, “Mary divorced John”, same John?
    <ul>
      <li>the solution is to have a canonical representation and then create
role-specific representations with composition functions.</li>
    </ul>
  </li>
</ul>

<h2 id="choosing-role-specific-representations">Choosing Role-Specific Representations</h2>

<p>We are now dealing with family trees, where instances take the form:</p>

<p>person1 relationship person2</p>

<p>Hinton introduces an extra group of neurons above the role and relationship
representations. This is responsible to looking at interactions between the
participating representations. It can perform “micro-inferences” which is
useful for pattern completion. For example, if the entity of person1 is “old”
and the relationship is “has-husband”, and given a missing person2, we can
infer that person2 must also be “old” and activate that neuron.</p>

<p>Note it is a strength of componential representations that we can
do such a thing.</p>

<p>Microinferences can be seen as storing the regularities inherent in a domain
and providing sensible generalization. This is a good criterion for a
representation: that it allow us to express the regularities of a domain.</p>

<p>Returning to this feature neuron for “old”. It may be asked whether the network
understands what “old” really means. Philosophically, that is a difficult
question since we don’t know what a meaning is. However, sticking with a
truth-condition view, which is perhaps useful to get started, we can say
that if the network correctly partitions the set of all entities into
old and not, and this neuron is active for all old entities, then it has
captured this meaning.</p>

<p>The search for good features can then be viewed as the search for the
most useful and relevant partitions. Note that with continuous values this
allows “fuzzy” partitions. Indeed an advantage. But that does make the
apparent discreteness of biological neurons more of a mystery.</p>


  </div>

  
</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <!--<h2 class="footer-heading">Tim Niven (寒山)</h2>-->

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>
            
              Tim Niven (寒山)
            
            </li>
            
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/timniven"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">timniven</span></a>

          </li>
          

          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>I maintain this blog to share information pertaining to my research that may be of interest to others, as well as generally useful information I wish to make a note of.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
