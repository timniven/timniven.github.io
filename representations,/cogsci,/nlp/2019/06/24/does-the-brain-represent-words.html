<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Does the brain represent words? | Tim Niven (寒山)</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Does the brain represent words?" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The paper is by Jon Gauthier and Anna Ivanova, and is from June 2018." />
<meta property="og:description" content="The paper is by Jon Gauthier and Anna Ivanova, and is from June 2018." />
<link rel="canonical" href="http://localhost:4000/representations,/cogsci,/nlp/2019/06/24/does-the-brain-represent-words.html" />
<meta property="og:url" content="http://localhost:4000/representations,/cogsci,/nlp/2019/06/24/does-the-brain-represent-words.html" />
<meta property="og:site_name" content="Tim Niven (寒山)" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-06-24T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"The paper is by Jon Gauthier and Anna Ivanova, and is from June 2018.","@type":"BlogPosting","headline":"Does the brain represent words?","dateModified":"2019-06-24T00:00:00+08:00","datePublished":"2019-06-24T00:00:00+08:00","url":"http://localhost:4000/representations,/cogsci,/nlp/2019/06/24/does-the-brain-represent-words.html","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/representations,/cogsci,/nlp/2019/06/24/does-the-brain-represent-words.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Tim Niven (寒山)" /></head>


  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  <body>

    <header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Tim Niven (寒山)</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/publications/">Publications</a></div>
      </nav></div>
</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Does the brain represent words?</h1>
    <p class="post-meta"><time datetime="2019-06-24T00:00:00+08:00" itemprop="datePublished">Jun 24, 2019</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>The <a href="https://arxiv.org/abs/1806.00591">paper</a> is by Jon Gauthier and Anna 
Ivanova, and is from June 2018.</p>

<p>My interest in this paper comes from the claim made therein that work in NLP on 
universal representations appears to be on the right track.</p>

<h2 id="brief-summary-of-the-paper">Brief Summary of the Paper</h2>

<p>The seminal work of Mitchell et al. (2008) used a trillion word 
corpus to define semantic representations on words based on co-occurrence with 
a specifically chosen set of 25 sensorimotor verbs thought to be associated 
with semantic representation: see, hear, listen, taste… they found this was 
significantly useful to predict neural activation patterns over nine subjects.</p>

<p>As an aside, I thought one of the more interesting parts of that paper was the 
suggestion that a neural representation could be obtained as a linear 
superposition of representations from sub-modules.</p>

<p>Following this, other researchers have been looking for better feature spaces, 
based on, e.g. behavioural ratings and distributional statistics. An extension 
has also been made to sentence decoding.</p>

<p>A decoding study is described as follows</p>
<ul>
  <li>goal: derive a set of stimulus-specific linguistic features and measure how 
it is associated with brain activity</li>
  <li>method: see if the brain activity patterns can predict the chosen features</li>
  <li>conclusion: if the features reflect semantic properties of the stimulus, then 
the brain activity pattern is considered a “semantic representation”</li>
</ul>

<p>The authors of this paper argue for the claim that</p>

<blockquote>
  <p>such talk of representation is meaningless unless one also specifies the 
brain mechanisms utilizing those representations and the task they are designed 
to solve.</p>
</blockquote>

<p>since such representational claims</p>

<blockquote>
  <p>wildly over-generate, leading us to award the label of “representation” to 
brain activity evoked by any arbitrary aspect of the stimulus, so long as it 
has some vague relation to the stimulus “meaning”</p>
</blockquote>

<p>A specific example of “over-generation”: the study of Pereira et al. (2018), 
wherein fMRI data from subjects reading a sentence was used to predict the 
embeddings of the words in that sentence, claimed that their decoder could 
read out “linguistic meaning”.</p>

<p>But since word embeddings have been shown at best to capture a limited range 
of things such as “elements of syntax” and “hypernymy relations”</p>

<blockquote>
  <p>we could just as well claim that the decoder has captured “elements of 
syntax” or “hypernymy relations.”</p>
</blockquote>

<p>and since we do more than reason about syntax and hypernymy relations when 
reading a sentence, this underdetermines the the nature and function of neural 
computations.</p>

<p>Furthermore, representations do not exist in a vacuum: they are created by some
part of the brain to be potentially consumed by another part and produce
behaviour. (Some interesting references to philosophical work I would like to
read on this point: Papineau, 1992; Dretske, 1995).</p>

<p>So, the authors re-run the experiments of Pereira et al. (2018) by learning a 
decoder that maps the fMRI data to the neural representations from models 
trained to perform specific tasks.</p>

<p>All neural models perform above chance, and the best performance is achieved 
by those that are more general (e.g. GloVe and NLI).</p>

<p>These results are where the suggestion comes from - the more general NLP model 
the better the fMRIs can predict its representations.</p>

<h2 id="thoughts">Thoughts</h2>

<h3 id="general-representations">General Representations</h3>

<p>That fMRIs better predict the features of more general NLP 
representations is an interesting idea. On the face of it, it seems quite 
reasonable. Assume we take this as an evaluation criteria. Some following 
questions:</p>
<ul>
  <li>How does BERT do?</li>
  <li>How do multimodal representations do?</li>
</ul>

<h3 id="language-understanding">Language Understanding</h3>

<p>The criticism of previous work on decoders echoes a concern of mine in NLP with 
claims of “language understanding” following success at a supervised learning 
task (such as natural language inference). Even if you tick off all the items
in the evaluation</p>

<blockquote>
  <p>we could just as well claim that the network has captured elements of 
[what is on that list]</p>
</blockquote>

<p>as opposed to some general claim to language understanding. And that claim to
language understanding, such as it is, looks all the more suspect due to the
brittleness and susceptibility to simple adversaries, as I seem to have proven
to myself by poking around, and ample prior work demonstrates. 
However, the claim to “general language understanding “is much stronger for a 
multi-task learning setup (like GLUE). A criticism of GLUE might follow from 
doubting just how general it really is. Do the laundry list of tasks it covers
really cover everything we want to call language understanding? I don’t think 
so. If you can perform well on all the tasks but still not answer simple 
questions like in the Winograd challenge, then I think you don’t have it.
In short, I think language understanding fundamentally includes the common
sense everyone is talking about trying to get into our models, which is a 
function of learning grounded in the world.</p>

<p>So that’s where my CCQs project comes in, and where it should help this 
situation - by providing a more direct test for what we consider “understanding”
to be.</p>

  </div>

  
</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <!--<h2 class="footer-heading">Tim Niven (寒山)</h2>-->

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>
            
              Tim Niven (寒山)
            
            </li>
            
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/timniven"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">timniven</span></a>

          </li>
          

          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>Information and ideas pertaining to my research and other interests.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
