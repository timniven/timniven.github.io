<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Exploitation of spurious statistics undermines an ‘Engineering Success’ argument for less innate structure | Tim Niven (寒山)</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Exploitation of spurious statistics undermines an ‘Engineering Success’ argument for less innate structure" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In a recent interview with Data Skeptic, the question of empiricism in contemporary artificial intelligence arose, and it gave me an opportunity to raise the issue of how the problem of models latching onto spurious statistical artifacts in our datasets connects to contemporary arguments for a strong form of empiricism. I would like to sketch the argument below in the hope it will attract feedback and criticism." />
<meta property="og:description" content="In a recent interview with Data Skeptic, the question of empiricism in contemporary artificial intelligence arose, and it gave me an opportunity to raise the issue of how the problem of models latching onto spurious statistical artifacts in our datasets connects to contemporary arguments for a strong form of empiricism. I would like to sketch the argument below in the hope it will attract feedback and criticism." />
<link rel="canonical" href="http://localhost:4000/representations,/cogsci,/nlp/2019/06/27/an-argument-against-less-innate-structure.html" />
<meta property="og:url" content="http://localhost:4000/representations,/cogsci,/nlp/2019/06/27/an-argument-against-less-innate-structure.html" />
<meta property="og:site_name" content="Tim Niven (寒山)" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-06-27T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"In a recent interview with Data Skeptic, the question of empiricism in contemporary artificial intelligence arose, and it gave me an opportunity to raise the issue of how the problem of models latching onto spurious statistical artifacts in our datasets connects to contemporary arguments for a strong form of empiricism. I would like to sketch the argument below in the hope it will attract feedback and criticism.","@type":"BlogPosting","headline":"Exploitation of spurious statistics undermines an ‘Engineering Success’ argument for less innate structure","dateModified":"2019-06-27T00:00:00+08:00","datePublished":"2019-06-27T00:00:00+08:00","url":"http://localhost:4000/representations,/cogsci,/nlp/2019/06/27/an-argument-against-less-innate-structure.html","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/representations,/cogsci,/nlp/2019/06/27/an-argument-against-less-innate-structure.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Tim Niven (寒山)" /></head>


  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  <body>

    <header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Tim Niven (寒山)</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/publications/">Publications</a></div>
      </nav></div>
</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Exploitation of spurious statistics undermines an &#39;Engineering Success&#39; argument for less innate structure</h1>
    <p class="post-meta"><time datetime="2019-06-27T00:00:00+08:00" itemprop="datePublished">Jun 27, 2019</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>In a recent interview with Data Skeptic, the question of empiricism in
contemporary artificial intelligence arose, and it gave me an opportunity to
raise the issue of how the problem of models latching onto spurious statistical
artifacts in our datasets connects to contemporary arguments for a strong form
of empiricism. I would like to sketch the argument below in the hope it will
attract feedback and criticism.</p>

<p>I don’t know all the reasons why Yann LeCun sees innate structure as an “evil” 
to be minimized. However, watching his debate with Gary Marcus, he appears to
offer what could be called an “argument from engineering success”:</p>

<blockquote>
  <p>(1) The less innate structure we put into our models, the better they have 
  performed</p>
</blockquote>

<blockquote>
  <p>(2) Engineering success is a strong indication of the right path</p>
</blockquote>

<blockquote>
  <p>(3) Therefore less innate structure is better</p>
</blockquote>

<p>We of course need to define what we mean by innate structure. From the same
debate, it appears LeCun and Marcus have different ideas of what this means.
Marcus argues that NIPS papers roundly ignore innate structure; LeCun states
exactly the opposite is the case. That’s a question I want to return to in the
future and think more deeply about.</p>

<p>Returning to the argument from engineering success. The problem of machine
learning models exploiting spurious statistics in the datasets on
which we measure this success undermines the argument at once because this is
very clearly not the notion of success we seek. This could of course be added
to well-known concerns of non-systematic and non-compositional learning,
brittleness to adversarial attacks, and failures to generalize beyond the
distribution of the training set in the ways that humans do, and that we seek in
and expect from strong models. However, here I just focus on the how findings of
models exploitating spurious statistics to attain near human performance
directly undermines (1).</p>

<p>There is a natural question as to just how big the problem really is - i.e. just
how much (1) is undermined. This seems to be an open question, and even one that
needs work to formulate precisely and meaningfully. I look forward to
attempting to address this question in the future.</p>

<p>We do appear to be able to say some things, though. I am
no expert in vision and speech recognition, but it seems to me that we have had
unquestionable success in parts of those areas. In language understanding,
however, my sense is not so positive. Since there are no current studies that
directly address this problem more generally this “sense” is something rather
subjective, being a distillation of the growing and significant work on the
problem so far, which has rather focused on specific datasets. This sense comes
in part from the following results</p>
<ul>
  <li>You can achieve a surprisingly high accuracy on NLI datasets with just the
hypothesis.  On SNLI, when you add the last verb of the premise, you are up to
76% already. We are far from enumerating all the heuristics and we only have
14% of performance left to account for. It is likely in my mind at least a
significant portion of that is going to be heurstics.</li>
  <li>In a forthcoming paper we collected all NLI heuristics measurements, and
our experiments show that for the best models out there (including the GLUE
winners) the better the performance, the more these heuristics are being
exploited.</li>
  <li>My own research on a much higher level NLP task (ARCT) that showed BERT’s
apparent near-human performance can be entirely accounted for by exploiting
spurious statistics, and that the true state of the art on this task for
neural networks remains random.</li>
</ul>

<p>This is of course likely to be a one-sided and very incomplete list, and I look
forward to hearing objections and counter-arguments that attempt to sharpen this
“sense” of mine. Adding supporting cases is also keenly welcomed - I haven’t
had the time to list some others I am aware of.</p>

<p>Returning to the argument, in the absence of a compelling general case that
demonstrates how serious the problem of spurious statistics is in NLP, perhaps
the correct conclusion to draw is</p>

<blockquote>
  <p>To the extent that our models are finding spurious solutions to our datasets
is the extent to which the argument from engineering success is undermined.</p>
</blockquote>

<p>This suggests it would be interesting to attempt to show one way or another just
how significant the problem is, and therefore how significantly the argument
from engineering is undermined by this problem. Or else, a general answer to
this question may not even make sense, and we are left collecting our “sense”
of the problem from individual cases.</p>

<p>Note also other similar arguments from different directions that undermine (1),
mentioned above: the extent to which what is learned is not systematic and
compositional, and fails to generate beyond the distribution of the training set
in the way that humans do, and that we expect and aim for. So the attack that
considers spurious statistics is not the only string in this bow.</p>

<p>As always, critical feedback is warmly encouraged. I still have not had the time
to turn commenting on, so please send me an email.</p>

  </div>

  
</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <!--<h2 class="footer-heading">Tim Niven (寒山)</h2>-->

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>
            
              Tim Niven (寒山)
            
            </li>
            
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/timniven"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">timniven</span></a>

          </li>
          

          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>Information and ideas pertaining to my research and other interests.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
