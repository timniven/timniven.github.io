<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-06-24T20:08:03+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Tim Niven (寒山)</title><subtitle>Information and ideas pertaining to my research and other interests.
</subtitle><entry><title type="html">Does the brain represent words?</title><link href="http://localhost:4000/representations,/cogsci,/nlp/2019/06/24/does-the-brain-represent-words.html" rel="alternate" type="text/html" title="Does the brain represent words?" /><published>2019-06-24T00:00:00+08:00</published><updated>2019-06-24T00:00:00+08:00</updated><id>http://localhost:4000/representations,/cogsci,/nlp/2019/06/24/does-the-brain-represent-words</id><content type="html" xml:base="http://localhost:4000/representations,/cogsci,/nlp/2019/06/24/does-the-brain-represent-words.html">&lt;p&gt;The &lt;a href=&quot;https://arxiv.org/abs/1806.00591&quot;&gt;paper&lt;/a&gt; is by Jon Gauthier and Anna 
Ivanova, and is from June 2018.&lt;/p&gt;

&lt;p&gt;My interest in this paper comes from the claim made therein that work in NLP on 
universal representations appears to be on the right track.&lt;/p&gt;

&lt;h2 id=&quot;brief-summary-of-the-paper&quot;&gt;Brief Summary of the Paper&lt;/h2&gt;

&lt;p&gt;The seminal work of Mitchell et al. (2008) used a trillion word 
corpus to define semantic representations on words based on co-occurrence with 
a specifically chosen set of 25 sensorimotor verbs thought to be associated 
with semantic representation: see, hear, listen, taste… they found this was 
significantly useful to predict neural activation patterns over nine subjects.&lt;/p&gt;

&lt;p&gt;As an aside, I thought one of the more interesting parts of that paper was the 
suggestion that a neural representation could be obtained as a linear 
superposition of representations from sub-modules.&lt;/p&gt;

&lt;p&gt;Following this, other researchers have been looking for better feature spaces, 
based on, e.g. behavioural ratings and distributional statistics. An extension 
has also been made to sentence decoding.&lt;/p&gt;

&lt;p&gt;A decoding study is described as follows&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;goal: derive a set of stimulus-specific linguistic features and measure how 
it is associated with brain activity&lt;/li&gt;
  &lt;li&gt;method: see if the brain activity patterns can predict the chosen features&lt;/li&gt;
  &lt;li&gt;conclusion: if the features reflect semantic properties of the stimulus, then 
the brain activity pattern is considered a “semantic representation”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The authors of this paper argue for the claim that&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;such talk of representation is meaningless unless one also specifies the 
brain mechanisms utilizing those representations and the task they are designed 
to solve.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;since such representational claims&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;wildly over-generate, leading us to award the label of “representation” to 
brain activity evoked by any arbitrary aspect of the stimulus, so long as it 
has some vague relation to the stimulus “meaning”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A specific example of “over-generation”: the study of Pereira et al. (2018), 
wherein fMRI data from subjects reading a sentence was used to predict the 
embeddings of the words in that sentence, claimed that their decoder could 
read out “linguistic meaning”.&lt;/p&gt;

&lt;p&gt;But since word embeddings have been shown at best to capture a limited range 
of things such as “elements of syntax” and “hypernymy relations”&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;we could just as well claim that the decoder has captured “elements of 
syntax” or “hypernymy relations.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;and since we do more than reason about syntax and hypernymy relations when 
reading a sentence, this underdetermines the the nature and function of neural 
computations.&lt;/p&gt;

&lt;p&gt;Furthermore, representations do not exist in a vacuum: they are created by some
part of the brain to be potentially consumed by another part and produce
behaviour. (Some interesting references to philosophical work I would like to
read on this point: Papineau, 1992; Dretske, 1995).&lt;/p&gt;

&lt;p&gt;So, the authors re-run the experiments of Pereira et al. (2018) by learning a 
decoder that maps the fMRI data to the neural representations from models 
trained to perform specific tasks.&lt;/p&gt;

&lt;p&gt;All neural models perform above chance, and the best performance is achieved 
by those that are more general (e.g. GloVe and NLI).&lt;/p&gt;

&lt;p&gt;These results are where the suggestion comes from - the more general NLP model 
the better the fMRIs can predict its representations.&lt;/p&gt;

&lt;h2 id=&quot;thoughts&quot;&gt;Thoughts&lt;/h2&gt;

&lt;h3 id=&quot;general-representations&quot;&gt;General Representations&lt;/h3&gt;

&lt;p&gt;That fMRIs better predict the features of more general NLP 
representations is an interesting idea. On the face of it, it seems quite 
reasonable. Assume we take this as an evaluation criteria. Some following 
questions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;How does BERT do?&lt;/li&gt;
  &lt;li&gt;How do multimodal representations do?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;language-understanding&quot;&gt;Language Understanding&lt;/h3&gt;

&lt;p&gt;The criticism of previous work on decoders echoes a concern of mine in NLP with 
claims of “language understanding” following success at a supervised learning 
task (such as natural language inference). Even if you tick off all the items
in the evaluation&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;we could just as well claim that the network has captured elements of 
[what is on that list]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;as opposed to some general claim to language understanding. And that claim to
language understanding, such as it is, looks all the more suspect due to the
brittleness and susceptibility to simple adversaries, as I seem to have proven
to myself by poking around, and ample prior work demonstrates. 
However, the claim to “general language understanding “is much stronger for a 
multi-task learning setup (like GLUE). A criticism of GLUE might follow from 
doubting just how general it really is. Do the laundry list of tasks it covers
really cover everything we want to call language understanding? I don’t think 
so. If you can perform well on all the tasks but still not answer simple 
questions like in the Winograd challenge, then I think you don’t have it.
In short, I think language understanding fundamentally includes the common
sense everyone is talking about trying to get into our models, which is a 
function of learning grounded in the world.&lt;/p&gt;

&lt;p&gt;So that’s where my CCQs project comes in, and where it should help this 
situation - by providing a more direct test for what we consider “understanding”
to be.&lt;/p&gt;</content><author><name></name></author><summary type="html">The paper is by Jon Gauthier and Anna Ivanova, and is from June 2018.</summary></entry></feed>